{
    "collab_server" : "",
    "contents" : "---\ntitle: 'From Texts to Corpora Comparison'\nauthor: \"Shu-Kai Hsieh\"\ndate: \"2015/1/1\"\noutput:\n  html_document: default\nbibliography: /Users/shukai/LOPE_LexInfo/BIB/corpus.bib\n---\n\n\n\n# Introduction\n\n- **Text** may derive from virtually any source, e.g., newspaper articles, entries in wikipedia, science texts in schools, legal and govenment documents, short stories, or advertisements.\n\n- our approach is to analyze corpora on many levels. This app will unveil the many ways that corpora vary in different contexts.\n\n- 語料庫語言學研究需要回答以下的方法論問題：\n    - 同一研究主題，使用不同語料庫的效應。\n    - 不同主題，所需要的樣本資料。\n    - 跨語言比較所使用的語料庫基準 baseline。\n\n\n# Corpora comparison: reviews\n\nThe unit will cover different aspects of corpus analysis \nregarding the comparision of corpora/subcorpora in the area of \nregister analysis, contrastive analysis (mostly German English) \nincluding choosing the right corpora and features, \nspecific aspects of corpus query, post-processing of extracted \nlinguistic evidence, statistical analysis, classification and \nvisualization. \n\n\n\n\n\n\n\n## Query in Text Corpora\nThe unit will cover text encoding, character encoding, \nregular expressions, search with regular expressions, \nsearch in unannotated corpora, simple text search, \nand search in annotated corpora with a corpus query \nlanguage (for instance CQP) and search in XML documents using XQuery. \n\n- text encoding; the most popular standards of text encoding will be\npresented,\n- regular expressions; the concept of a regular expression will be introduced; Students will have a chance to discover the utility of regular expressions and to learn how to formulate regular expressions corresponding to their queries.\n- annotation; different kinds of annotation (structural mark-up, part-of-speech tagging, morphosyntactic annotation, parsing) and different annotation schemes will be presented and their utility for different kinds of research questions will be discussed.\n- the exploration of language corpora. \n\n\n## Measures\n\nA measure of corpus similarity would be very useful for lexicography\n\nWord frequency lists are cheap and easy to generate so a measure based on \nthem can be used where a detailed comparison of the two corpora is not viable\n\nused the method proposed in Kilgarriff (2001) to assess corpus similarity over \na short period of time both within topic and cross topic. \n\n\n## 實作問題\n- data size 太大，難以線上分析？（所以小文本可上傳，大語料庫則上傳特定格式）\n\n-----------------------\n\n## 斷詞問題導致的計量問題有多嚴重？\n\n用不同指標比較看看\n\n\n```{r}\nlibrary(zipfR)\n```\n\n### 先用構詞衍生力來看\n\nCase 1: Estimating vocabulary size of a productive process (ri- and 重)\n\n\nThe data were extracted from a 380 million token\ncorpus of newspaper Italian (Baroni et al., 2004), \nand they consist of all verbal lemmas beginning with the \nprefix ri- in the corpus (extracted with a mixture of \nautomated methods and manual checking).\n\n* Frequency spectra\n```{r eval=FALSE}\ndata(ItaRi.spc)\nsummary(ItaRi.spc)\nVm(ItaRi.spc,1:5)\n\n# \"Baayen's P measure\" of productivity proposed by Harald Baayen (see, e.g., Baayen, 1992).\nVm(ItaRi.spc,1) / N(ItaRi.spc)\npar(mfrow=c(1,3))\nplot(ItaRi.spc)\nplot(ItaRi.spc, log=\"x\")\nplot.default(ItaRi.spc, main=\"Frequency Spectrum\")\n\n#A spectrum is often characterized by very high\n# values corresponding to the lowest frequency classes, \n# and a very long tail of frequency classes with only one \n# member (i.e., just one word with frequency 100, \n# just one word with frequency 103, etc.)\nVm(ItaRi.spc,100)\n# ItaRi.spc$Vm[20]\n```\n\n\n\n* Vocabulary growth curves\n```{r eval=FALSE}\ndata(ItaRi.emp.vgc)\nhead(ItaRi.emp.vgc)\nsummary(ItaRi.emp.vgc)\n\npar(mfrow=c(1,2))\nplot(ItaRi.emp.vgc, add.m=1)\n```\n\n\n\n* Interpolation\n```{r eval=FALSE}\nItaRi.bin.vgc <- vgc.interp(ItaRi.spc, N(ItaRi.emp.vgc),m.max=1)\nhead(ItaRi.bin.vgc)\n\nplot(ItaRi.emp.vgc,ItaRi.bin.vgc,\n     legend=c(\"observed\",\"interpolated\"))\n\n```\n\n\n* Estimating V and other quantities at arbitrary sample sizes \n\n\nExtrapolate V to larger sample > LNRE model\nSupporting 3 models: \n1. Generalized Inverse Gauss Poisson (lnre.gigp)\n2. Zipf Mandelbrot (lnre.zm) \n3. finite Zipf Mandelbrot (lnre.fzm)\n\n```{r eval=FALSE}\n\nItaRi.fzm <- lnre(\"fzm\", ItaRi.spc, exact=FALSE)\n#ItaRi.zm <- lnre(\"zm\", ItaRi.spc, exact=FALSE)\n#ItaRi.gigp <- lnre(\"gigp\", ItaRi.spc, exact=FALSE)\nsummary(ItaRi.fzm)\n\nItaRi.fzm.spc <- lnre.spc(ItaRi.fzm, N(ItaRi.fzm))\n\nplot(ItaRi.spc,ItaRi.fzm.spc,legend=c(\"observed\",\"fZM\"))\n\nItaRi.fzm.vgc <- lnre.vgc(ItaRi.fzm, (1:100)*28e+3)\n\nplot(ItaRi.emp.vgc,ItaRi.fzm.vgc,N0=N(ItaRi.fzm), \n     legend=c(\"observed\",\"fZM\"))\n```\n\n```{r eval=FALSE}\n## Evaluating extrapolation quality\nItaRi.sub.spc <- sample.spc(ItaRi.spc, N=700000)\nItaRi.sub.fzm <- lnre(\"fzm\", ItaRi.sub.spc, exact=FALSE)\nItaRi.sub.fzm\n\nItaRi.sub.fzm.vgc <- lnre.vgc(ItaRi.sub.fzm,N=N(ItaRi.emp.vgc))\n\nplot(ItaRi.bin.vgc, ItaRi.sub.fzm.vgc, N0=N(ItaRi.sub.fzm),\n     legend=c(\"interpolated\",\"fZM\"))\n```\n\n\n\n* Comparing vocabulary growth curves of different categories\n\n```{r eval=FALSE}\ndata(ItaUltra.spc)\nsummary(ItaUltra.spc)\n\nItaUltra.fzm <- lnre(\"fzm\",ItaUltra.spc,exact=FALSE)\n\nItaUltra.fzm \nItaUltra.ext.vgc <- lnre.vgc(ItaUltra.fzm,N(ItaRi.emp.vgc))\nplot(ItaUltra.ext.vgc,ItaRi.bin.vgc,\n     legend=c(\"ultra-\",\"ri-\"))\n\n\n# Interestingly, the extrapolation suggests that ultra-, \n# while occurring more rarely, has the potential to generate \n# many more words than the already productive ri- process.\n\n\n## Now let's try different dataset\ndata(package=\"zipfR\")\n\n## 比較英文Brown corpus (read.tf1, tf12spc to generate a spc object)\n## 比較 TMC/ASBC/GIGAWORD/WEIBO only word-freq list required!!\n```\n\n*****************\n### Case 2: Estimating lexical coverage\n\nTo estimate the proportion of OOV words/types that we will encounter, given a lexicon of a certain size, or, from a different angle, to determine how large our lexicon should be in order to keep the OOV proportion below a certain threshold.\n\n\n1. Estimating the proportion of OOV types and tokens\ngiven a fixed size lexicon\n```{r eval=FALSE}\ndata(Brown100k.spc)\nsummary(Brown100k.spc)\n\nVseen <- V(Brown100k.spc) - Vm(Brown100k.spc,1)\nVseen\nVseen / V(Brown100k.spc)\n\nVm(Brown100k.spc,1) / N(Brown100k.spc)\n\nBrown100k.zm <- lnre(\"zm\", Brown100k.spc)\nBrown100k.zm\n\n\nEV(Brown100k.zm, c(1e6, 10e6,100e6))\nVseen / EV(Brown100k.zm, c(1e6,10e6,100e6))\n1 - (Vseen / EV(Brown100k.zm, c(1e6,10e6,100e6)))\n\n\ndata(Brown.spc)\nEV(Brown100k.zm,N(Brown.spc))\n\n1 - (Vseen / V(Brown.spc))\n1 - (Vseen / EV(Brown100k.zm, N(Brown.spc)))\n\n\nBrown.zm.spc <- lnre.spc(Brown100k.zm, N(Brown.spc))\n\nEV(Brown100k.zm, N(Brown.spc)) - Vseen\n\nsum(Vm(Brown.zm.spc, 1))\nsum(Vm(Brown.zm.spc, 1:2))\nsum(Vm(Brown.zm.spc, 1:6))\n\nNoov.zm <- sum(Vm(Brown.zm.spc, 1:6) * c(1:6))\nNoov.zm\n\nNoov.zm / N(Brown.spc)\nV(Brown.spc) - Vseen\n\n\nsum(Vm(Brown.spc, 1))\nsum(Vm(Brown.spc, 1:2))\nsum(Vm(Brown.spc, 1:13))\n\nNoov.emp <- sum(Vm(Brown.spc, 1:13) * c(1:13))\nNoov.emp\nNoov.emp / N(Brown.spc)\n```\n\n\n\n2. Determining lexicon size\n\n```{r eval=FALSE}\nBrown10M.zm.spc <- lnre.spc(Brown100k.zm, 10e6)\nsum(Vm(Brown10M.zm.spc, 1:18) * c(1:18))\nsum(Vm(Brown10M.zm.spc, 1:18))\n\nEV(Brown100k.zm, 10e6) - sum(Vm(Brown10M.zm.spc, 1:18))\n```\n\n**************\n\n3. DSM\n從 word distribution model 去比較 Brown corpus, ASBS/Gigaword/Plurk /COPENS\n\n\n```{r eval=FALSE}\n## COPENS \n\n# sum(copens[2])  # 5113481 tokens \ncopens <- read.csv(\"~/LOPEN/COPENS/COPENS.workshop/fdist_all.csv\")\nnames(copens) <-c(\"WORD\", \"RAW\", \"FREQ\")\ncopens.tbl <- copens[1:2]\n\n## tokens\nsum(copens[2])\n\ncopens.vec <- copens[1:137874,2]\ncopens.tfl <- vec2tfl(copens.vec)\ncopens.spc <- vec2spc(copens.vec)\ncopens.vgc <- vec2vgc(copens.vec)\nsummary(copens.spc)\n\nVm(copens.spc, 1:5)\nN(copens.spc)\nV(copens.spc)\nplot(copens.spc)\n\nhead(copens.vgc)\nsummary(copens.vgc)\n#plot(copens.vgc, add.m=1)\n\ncopens.bin.vgc <- vgc.interp(copens.spc, \n                           N(copens.vgc),m.max=1)\nhead(copens.bin.vgc)\nplot(copens.vgc, copens.bin.vgc,\n     legend=c(\"observed\",\"interpolated\"))\n```\n\n\nLIVAC 實驗 \n\n\n```{r eval=FALSE}\n## Brown Corpus 一百萬詞目\ndata(Brown.spc)\nsummary(Brown.spc)\n\n## ASBC 4.0 一千萬詞目(1981 年到 2007)\nasbc.spc <- read.spc(\"asbc/asbc_frequency_spectrum.txt\")\nsummary(asbc.spc)\nVm(asbc.spc,1:5)\n\n## LDC Chinese Gigaword Corpus 六億八千萬詞目\ngigaword.spc <- read.spc(\"LDC_gigaword/gigaword_frequency_spectrum.txt\")\nsummary(gigaword.spc)\nVm(gigaword.spc,1:5)\n\n## NTU Plurk corpus 兩千七百萬詞目\nplurk.spc <- read.spc(\"~/corpus/Comparison/plurk_frequency_spectrum.txt\")\nsummary(plurk.spc)\nVm(plurk.spc,1:5)\n\nVm(livac.spc,1:10)\n\n## 先把四個語料庫的 N 與 V 並列的 plot 出來\nN(Brown.spc)\nN(asbc.spc) \nV(asbc.spc)\nN(plurk.spc)\nV(plurk.spc)\n\n\n####### LIVAC\nlivac.spc <- read.spc(\"livac/hk_spectrum.txt\")\nsummary(livac.spc)\nVm(livac.spc,1:5)\nN(livac.spc)\nV(livac.spc)\n```\n\n- 比較 Baayen P\n```{r eval=FALSE}\n# \"Baayen's P measure\" of productivity proposed by Harald Baayen (see, e.g., Baayen, 1992).\nVm(Brown.spc,1) / N(Brown.spc)\nVm(gigaword.spc,1)/N(gigaword.spc)\n\npar(mfrow=c(2,2))\nplot(Brown.spc, log= \"x\",main=\"Brown\")\n#plot(Brown.spc, log=\"x\")\n#plot.default(Brown.spc, main=\"Frequency Spectrum\")\nplot(asbc.spc, log= \"x\", main=\"ASBC\")\nplot(gigaword.spc, log= \"x\",main=\"LDC.Gigaword\")\nplot(plurk.spc)\n\n###\nplot(livac.spc, log= \"x\",main=\"LIVAC\")\n```\n\n- 利用 VGC 來比較\n```{r eval=FALSE}\n\npar(mfrow=c(2,1))\n# Brown\ndata(Brown.emp.vgc)\nhead(Brown.emp.vgc)\nsummary(Brown.emp.vgc)\nplot(Brown.emp.vgc, add.m=1, main=\"Brown\")\n\n\n# ASBC \nasbc.vgc <- read.vgc(\"asbc/asbc_vg.txt\")\nhead(asbc.vgc)\nsummary(asbc.vgc)\nplot(asbc.vgc, add.m=1, main = \"ASBC\")\n\n# Gigaword\ngigaword.vgc <- read.vgc(\"LDC_gigaword/gigaword_vg.txt\")\nhead(gigaword.vgc)\nsummary(gigaword.vgc)\nplot(gigaword.vgc, add.m=1, main = \"LDC_Gigaword\")\n\n# Plurk\n# plurk.vgc <- read.vgc(\"~/corpus/Comparison/plurk_vg.txt\")\n# head(asbc.vgc)\n# summary(plurk.vgc)\n# plot(plurk.vgc, add.m=1)\n\n# LIVAC\nlivac.vgc <- read.vgc(\"~/Dropbox/Linguistic.Data/corpus.comparison/livac/hk_vg.txt\")\nhead(livac.vgc)\nsummary(livac.vgc)\nplot(livac.vgc, add.m=1, main = \"LIVAC\")\n\n\n\n```\n\n\n\n\n- 加入 Baayen proposed 的 binomial interpolation 來看\n```{r eval=FALSE}\n## Brown\nBrown.bin.vgc <- vgc.interp(Brown.spc, \n                                    N(Brown.emp.vgc),m.max=1)\nhead(Brown.bin.vgc)\nplot(Brown.emp.vgc, Brown.bin.vgc,\n     legend=c(\"observed\",\"interpolated\"))\n\n## ASBC\nasbc.bin.vgc <- vgc.interp(asbc.spc, \n                            N(asbc.vgc),m.max=1)\nhead(asbc.bin.vgc)\nplot(asbc.vgc, asbc.bin.vgc,\n     legend=c(\"observed\",\"interpolated\"))\n\n## Gigaword\ngigaword.bin.vgc <- vgc.interp(gigaword.spc, \n                           N(gigaword.vgc),m.max=1)\nhead(asbc.bin.vgc)\n\n\nplot(gigaword.vgc, gigaword.bin.vgc,\n     legend=c(\"observed\",\"interpolated\"), main = \"LDC.Gigaword\")\n\n## Plurk\nplurk.bin.vgc <- vgc.interp(plurk.spc, \n                               N(plurk.vgc),m.max=1)\nhead(plurk.bin.vgc)\nplot(plurk.vgc, plurk.bin.vgc,\n     legend=c(\"observed\",\"interpolated\"))\n\n\n## LIVAC\nlivac.bin.vgc <- vgc.interp(livac.spc, \n                               N(livac.vgc),m.max=1)\nhead(livac.bin.vgc)\nplot(livac.vgc, livac.bin.vgc,\n     legend=c(\"observed\",\"interpolated\"), main = \"LIVAC\")\n\n\n```\n\n- Estimating V and other quantities at arbitrary sample sizes \n\nExtrapolate V to larger sample > LNRE model\nSupporting 3 models: (1).Generalized Inverse Gauss Poisson (lnre.gigp)\n(2) Zipf Mandelbrot (lnre.zm) (3) finite Zipf Mandelbrot (lnre.fzm)\n\n```{r eval=FALSE}\n### Brown\nBrown.fzm <- lnre(\"fzm\", Brown.spc, exact=FALSE)\n#ItaRi.zm <- lnre(\"zm\", ItaRi.spc, exact=FALSE)\n#ItaRi.gigp <- lnre(\"gigp\", ItaRi.spc, exact=FALSE)\nsummary(Brown.fzm)\n\nBrown.fzm.spc <- lnre.spc(Brown.fzm, N(Brown.fzm))\nplot(Brown.spc, Brown.fzm.spc,legend=c(\"observed\",\"fZM\"))\n\nBrown.fzm.vgc <- lnre.vgc(Brown.fzm, (1:100)*28e+3)\n\nplot(Brown.emp.vgc, Brown.fzm.vgc, N0=N(Brown.fzm), \n     legend=c(\"observed\",\"fZM\"))\n\n\n### ASBC\nasbc.fzm <- lnre(\"fzm\", asbc.spc, exact=FALSE)\n#ItaRi.zm <- lnre(\"zm\", ItaRi.spc, exact=FALSE)\n#ItaRi.gigp <- lnre(\"gigp\", ItaRi.spc, exact=FALSE)\nsummary(asbc.fzm)\n\nasbc.fzm.spc <- lnre.spc(asbc.fzm, N(asbc.fzm))\nplot(asbc.spc, asbc.fzm.spc,legend=c(\"observed\",\"fZM\"))\n\nasbc.fzm.vgc <- lnre.vgc(asbc.fzm, (1:100)*1e9)\n\nplot(asbc.vgc, Brown.fzm.vgc, N0=N(asbc.fzm), \n     legend=c(\"observed\",\"fZM\"))\n\n### Gigaword\n\n\ngigaword.fzm <- lnre(\"fzm\", gigaword.spc, exact=FALSE)\n#ItaRi.zm <- lnre(\"zm\", ItaRi.spc, exact=FALSE)\n#ItaRi.gigp <- lnre(\"gigp\", ItaRi.spc, exact=FALSE)\nsummary(gigaword.fzm)\n\ngigaword.fzm.spc <- lnre.spc(gigaword.fzm, N(gigaword.fzm))\nplot(gigaword.spc, gigaword.fzm.spc,legend=c(\"observed\",\"fZM\"))\n\ngigaword.fzm.vgc <- lnre.vgc(gigaword.fzm, (1:100)*1e9)\n\nplot(gigaword.vgc, gigaword.fzm.vgc, N0=N(gigaword.fzm), \n     legend=c(\"observed\",\"fZM\"))\n\n\n### plurk\n\nplurk.fzm <- lnre(\"fzm\", plurk.spc, exact=FALSE)\n#ItaRi.zm <- lnre(\"zm\", ItaRi.spc, exact=FALSE)\n#ItaRi.gigp <- lnre(\"gigp\", ItaRi.spc, exact=FALSE)\nsummary(plurk.fzm)\n\nplurk.fzm.spc <- lnre.spc(plurk.fzm, N(plurk.fzm))\nplot(plurk.spc, plurk.fzm.spc,legend=c(\"observed\",\"fZM\"))\n\nplurk.fzm.vgc <- lnre.vgc(plurk.fzm, (1:100)*1e9)\n\nplot(plurk.vgc, plurk.fzm.vgc, N0=N(plurk.fzm), \n     legend=c(\"observed\",\"fZM\"))\n\n```\n\n\n```{r eval=FALSE}\n# ####### Evaluating extrapolation quality\n# ItaRi.sub.spc <- sample.spc(ItaRi.spc, N=700000)\n# ItaRi.sub.fzm <- lnre(\"fzm\", ItaRi.sub.spc, exact=FALSE)\n# ItaRi.sub.fzm\n# \n# ItaRi.sub.fzm.vgc <- lnre.vgc(ItaRi.sub.fzm,N=N(ItaRi.emp.vgc))\n# \n# plot(ItaRi.bin.vgc, ItaRi.sub.fzm.vgc, N0=N(ItaRi.sub.fzm),\n#      legend=c(\"interpolated\",\"fZM\"))\n# \n# \n# ## Comparing vocabulary growth curves of different categories\n# data(ItaUltra.spc)\n# summary(ItaUltra.spc)\n# \n# ItaUltra.fzm <- lnre(\"fzm\",ItaUltra.spc,exact=FALSE)\n# \n# ItaUltra.fzm \n# ItaUltra.ext.vgc <- lnre.vgc(ItaUltra.fzm,N(ItaRi.emp.vgc))\n# plot(ItaUltra.ext.vgc,ItaRi.bin.vgc,\n#      legend=c(\"ultra-\",\"ri-\"))\n# \n# \n# # Interestingly, the extrapolation suggests that ultra-, \n# # while occurring more rarely, has the potential to generate \n# # many more words than the already productive ri- process.\n```\n\n\n```{r eval=FALSE}\n## 看看 plurk 詞表。朱學 Na 恆 D\nplurk_word_list <-read.table(\"~/corpus/Comparison/plurk_word_list.txt\", header=F)\n\n\n## Now let's try different dataset\ndata(package=\"zipfR\")\n\n## 比較英文Brown corpus (read.tf1, tf12spc to generate a spc object)\n## 比較 TMC/ASBC/GIGAWORD/WEIBO only word-freq list required!!\n\n\n#################\nCase 2: Estimating lexical coverage\n\n# to estimate the proportion of OOV\nwords/types that we will encounter, given a lexicon of a certain size, or,\nfrom a different angle, to determine how large our lexicon should be in order\nto keep the OOV proportion below a certain threshold.\n\n\n# [1] Estimating the proportion of OOV types and tokens\n# given a fixed size lexicon\n\ndata(Brown100k.spc)\nsummary(Brown100k.spc)\n\nVseen <- V(Brown100k.spc) - Vm(Brown100k.spc,1)\nVseen\nVseen / V(Brown100k.spc)\n\nVm(Brown100k.spc,1) / N(Brown100k.spc)\n\nBrown100k.zm <- lnre(\"zm\", Brown100k.spc)\nBrown100k.zm\n\n\nEV(Brown100k.zm, c(1e6, 10e6,100e6))\nVseen / EV(Brown100k.zm, c(1e6,10e6,100e6))\n1 - (Vseen / EV(Brown100k.zm, c(1e6,10e6,100e6)))\n\n\ndata(Brown.spc)\nEV(Brown100k.zm,N(Brown.spc))\n\n1 - (Vseen / V(Brown.spc))\n1 - (Vseen / EV(Brown100k.zm, N(Brown.spc)))\n\n\n####\n\n# 200k lemma tokens\nasbc.sub.spc <- sample.spc(asbc.spc, N=200000)\n\n\n\n# \n# \n# \n# Brown.zm.spc <- lnre.spc(Brown100k.zm, N(Brown.spc))\n# \n# EV(Brown100k.zm, N(Brown.spc)) - Vseen\n# \n# sum(Vm(Brown.zm.spc, 1))\n# sum(Vm(Brown.zm.spc, 1:2))\n# sum(Vm(Brown.zm.spc, 1:6))\n# \n# Noov.zm <- sum(Vm(Brown.zm.spc, 1:6) * c(1:6))\n# Noov.zm\n# \n# Noov.zm / N(Brown.spc)\n# V(Brown.spc) - Vseen\n# \n# \n# sum(Vm(Brown.spc, 1))\n# sum(Vm(Brown.spc, 1:2))\n# sum(Vm(Brown.spc, 1:13))\n# \n# Noov.emp <- sum(Vm(Brown.spc, 1:13) * c(1:13))\n# Noov.emp\n# Noov.emp / N(Brown.spc)\n# \n\n[2] Determining lexicon size\n\nBrown10M.zm.spc <- lnre.spc(Brown100k.zm, 10e6)\nsum(Vm(Brown10M.zm.spc, 1:18) * c(1:18))\nsum(Vm(Brown10M.zm.spc, 1:18))\nEV(Brown100k.zm, 10e6) - sum(Vm(Brown10M.zm.spc, 1:18))\n\n\nplurk.zm.spc <- lnre.spc(plurk.zm, 10e6)\nsum(Vm(Brown10M.zm.spc, 1:18) * c(1:18))\nsum(Vm(Brown10M.zm.spc, 1:18))\nEV(Brown100k.zm, 10e6) - sum(Vm(Brown10M.zm.spc, 1:18))\n\n```\n\n\n\n# Stylometric Measures\n\n```{r, eval=FALSE}\nlibrary(stylo)\nsetwd(\"~/Dropbox/Linguistic.Data/corpus.comparison/stylo.experiment/\")\nreport = stylo()\nsummary(report)\nreport$table.with.all.freqs\n\n\n```\n\n\n## Find the Fingerprint\n\n- Simple Delta measure (Burrows 2002) very successful\n- Frequencies of 100 – 5,000 most frequent words (MFW) form a “fingerprint” of an author’s style\n\n\n\n\n\n\n\n",
    "created" : 1491521063289.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2298411001",
    "id" : "CB380D83",
    "lastKnownWriteTime" : 1465551874,
    "last_content_update" : 1465551874,
    "path" : "~/Dropbox/Linguistic.Data/corpus.comparison/corpora_compare.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}